
@ARTICLE{Sutton1999-nj,
  title   = "Policy gradient methods for reinforcement learning with function
             approximation",
  author  = "Sutton, Richard S and McAllester, David and Singh, Satinder and
             Mansour, Yishay",
  journal = "Adv. Neural Inf. Process. Syst.",
  volume  =  12,
  year    =  1999
}



@UNPUBLISHED{Shu2019-jf,
  title    = "Understanding Architectures Learnt by Cell-based Neural
              Architecture Search",
  author   = "Shu, Yao and Wang, Wei and Cai, Shaofeng",
  abstract = "Neural architecture search (NAS) searches architectures
              automatically for given tasks, e.g., image classification and
              language modeling. Improving the search efficiency and
              effectiveness has attracted increasing attention in recent years.
              However, few efforts have been devoted to understanding the
              generated architectures. In this paper, we first reveal that
              existing NAS algorithms (e.g., DARTS, ENAS) tend to favor
              architectures with wide and shallow cell structures. These
              favorable architectures consistently achieve fast convergence and
              are consequently selected by NAS algorithms. Our empirical and
              theoretical study further confirms that their fast convergence
              derives from their smooth loss landscape and accurate gradient
              information. Nonetheless, these architectures may not necessarily
              lead to better generalization performance compared with other
              candidate architectures in the same search space, and therefore
              further improvement is possible by revising existing NAS
              algorithms.",
  month    =  sep,
  year     =  2019
}

@INPROCEEDINGS{Zhou2020-dg,
  title     = "Theory-inspired path-regularized differential network
               architecture search",
  booktitle = "Proceedings of the 34th International Conference on Neural
               Information Processing Systems",
  author    = "Zhou, Pan and Xiong, Caiming and Socher, Richard and Hoi, Steven
               C H",
  abstract  = "Despite its high search efficiency, differential architecture
               search (DARTS) often selects network architectures with
               dominated skip connections which lead to performance
               degradation. However, theoretical understandings on this issue
               remain absent, hindering the development of more advanced
               methods in a principled way. In this work, we solve this problem
               by theoretically analyzing the effects of various types of
               operations, e.g. convolution, skip connection and zero
               operation, to the network optimization. We prove that the
               architectures with more skip connections can converge faster
               than the other candidates, and thus are selected by DARTS. This
               result, for the first time, theoretically and explicitly reveals
               the impact of skip connections to fast network optimization and
               its competitive advantage over other types of operations in
               DARTS. Then we propose a theory-inspired path-regularized DARTS
               that consists of two key modules: (i) a differential
               group-structured sparse binary gate introduced for each
               operation to avoid unfair competition among operations, and (ii)
               a path-depth-wise regularization used to incite search
               exploration for deep architectures that often converge slower
               than shallow ones as shown in our theory and are not well
               explored during search. Experimental results on image
               classification tasks validate its advantages.",
  publisher = "Curran Associates Inc.",
  number    = "Article 695",
  pages     = "8296--8307",
  series    = "NIPS'20",
  month     =  dec,
  year      =  2020,
  address   = "Red Hook, NY, USA",
  location  = "Vancouver, BC, Canada"
}


@ARTICLE{Linghu2022-yy,
  title         = "Quantum circuit architecture search on a superconducting
                   processor",
  author        = "Linghu, Kehuan and Qian, Yang and Wang, Ruixia and Hu,
                   Meng-Jun and Li, Zhiyuan and Li, Xuegang and Xu, Huikai and
                   Zhang, Jingning and Ma, Teng and Zhao, Peng and Liu, Dong E
                   and Hsieh, Min-Hsiu and Wu, Xingyao and Du, Yuxuan and Tao,
                   Dacheng and Jin, Yirong and Yu, Haifeng",
  abstract      = "Variational quantum algorithms (VQAs) have shown strong
                   evidences to gain provable computational advantages for
                   diverse fields such as finance, machine learning, and
                   chemistry. However, the heuristic ansatz exploited in modern
                   VQAs is incapable of balancing the tradeoff between
                   expressivity and trainability, which may lead to the
                   degraded performance when executed on the noisy
                   intermediate-scale quantum (NISQ) machines. To address this
                   issue, here we demonstrate the first proof-of-principle
                   experiment of applying an efficient automatic ansatz design
                   technique, i.e., quantum architecture search (QAS), to
                   enhance VQAs on an 8-qubit superconducting quantum
                   processor. In particular, we apply QAS to tailor the
                   hardware-efficient ansatz towards classification tasks.
                   Compared with the heuristic ansatze, the ansatz designed by
                   QAS improves test accuracy from 31\% to 98\%. We further
                   explain this superior performance by visualizing the loss
                   landscape and analyzing effective parameters of all ansatze.
                   Our work provides concrete guidance for developing variable
                   ansatze to tackle various large-scale quantum learning
                   problems with advantages.",
  month         =  jan,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "quant-ph",
  eprint        = "2201.00934"
}


@article{OpenAIGYMDBLP:journals/corr/BrockmanCPSSTZ16,
  author    = {Greg Brockman and
               Vicki Cheung and
               Ludwig Pettersson and
               Jonas Schneider and
               John Schulman and
               Jie Tang and
               Wojciech Zaremba},
  title     = {OpenAI Gym},
  journal   = {CoRR},
  volume    = {abs/1606.01540},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.01540},
  eprinttype = {arXiv},
  eprint    = {1606.01540},
  timestamp = {Fri, 08 Nov 2019 12:51:06 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/BrockmanCPSSTZ16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{Davies2021-xh,
  title    = "Advancing mathematics by guiding human intuition with {AI}",
  author   = "Davies, Alex and Veli{\v c}kovi{\'c}, Petar and Buesing, Lars and
              Blackwell, Sam and Zheng, Daniel and Toma{\v s}ev, Nenad and
              Tanburn, Richard and Battaglia, Peter and Blundell, Charles and
              Juh{\'a}sz, Andr{\'a}s and Lackenby, Marc and Williamson, Geordie
              and Hassabis, Demis and Kohli, Pushmeet",
  abstract = "The practice of mathematics involves discovering patterns and
              using these to formulate and prove conjectures, resulting in
              theorems. Since the 1960s, mathematicians have used computers to
              assist in the discovery of patterns and formulation of
              conjectures1, most famously in the Birch and Swinnerton-Dyer
              conjecture2, a Millennium Prize Problem3. Here we provide
              examples of new fundamental results in pure mathematics that have
              been discovered with the assistance of machine
              learning-demonstrating a method by which machine learning can aid
              mathematicians in discovering new conjectures and theorems. We
              propose a process of using machine learning to discover potential
              patterns and relations between mathematical objects,
              understanding them with attribution techniques and using these
              observations to guide intuition and propose conjectures. We
              outline this machine-learning-guided framework and demonstrate
              its successful application to current research questions in
              distinct areas of pure mathematics, in each case showing how it
              led to meaningful mathematical contributions on important open
              problems: a new connection between the algebraic and geometric
              structure of knots, and a candidate algorithm predicted by the
              combinatorial invariance conjecture for symmetric groups4. Our
              work may serve as a model for collaboration between the fields of
              mathematics and artificial intelligence (AI) that can achieve
              surprising results by leveraging the respective strengths of
              mathematicians and machine learning.",
  journal  = "Nature",
  volume   =  600,
  number   =  7887,
  pages    = "70--74",
  month    =  dec,
  year     =  2021,
  language = "en"
}

@ARTICLE{alphafold2Jumper2021-lw,
  title    = "Highly accurate protein structure prediction with {AlphaFold}",
  author   = "Jumper, John and Evans, Richard and Pritzel, Alexander and Green,
              Tim and Figurnov, Michael and Ronneberger, Olaf and
              Tunyasuvunakool, Kathryn and Bates, Russ and {\v Z}{\'\i}dek,
              Augustin and Potapenko, Anna and Bridgland, Alex and Meyer,
              Clemens and Kohl, Simon A A and Ballard, Andrew J and Cowie,
              Andrew and Romera-Paredes, Bernardino and Nikolov, Stanislav and
              Jain, Rishub and Adler, Jonas and Back, Trevor and Petersen, Stig
              and Reiman, David and Clancy, Ellen and Zielinski, Michal and
              Steinegger, Martin and Pacholska, Michalina and Berghammer, Tamas
              and Bodenstein, Sebastian and Silver, David and Vinyals, Oriol
              and Senior, Andrew W and Kavukcuoglu, Koray and Kohli, Pushmeet
              and Hassabis, Demis",
  abstract = "Proteins are essential to life, and understanding their structure
              can facilitate a mechanistic understanding of their function.
              Through an enormous experimental effort1-4, the structures of
              around 100,000 unique proteins have been determined5, but this
              represents a small fraction of the billions of known protein
              sequences6,7. Structural coverage is bottlenecked by the months
              to years of painstaking effort required to determine a single
              protein structure. Accurate computational approaches are needed
              to address this gap and to enable large-scale structural
              bioinformatics. Predicting the three-dimensional structure that a
              protein will adopt based solely on its amino acid sequence-the
              structure prediction component of the 'protein folding
              problem'8-has been an important open research problem for more
              than 50 years9. Despite recent progress10-14, existing methods
              fall far short of atomic accuracy, especially when no homologous
              structure is available. Here we provide the first computational
              method that can regularly predict protein structures with atomic
              accuracy even in cases in which no similar structure is known. We
              validated an entirely redesigned version of our neural
              network-based model, AlphaFold, in the challenging 14th Critical
              Assessment of protein Structure Prediction (CASP14)15,
              demonstrating accuracy competitive with experimental structures
              in a majority of cases and greatly outperforming other methods.
              Underpinning the latest version of AlphaFold is a novel machine
              learning approach that incorporates physical and biological
              knowledge about protein structure, leveraging multi-sequence
              alignments, into the design of the deep learning algorithm.",
  journal  = "Nature",
  volume   =  596,
  number   =  7873,
  pages    = "583--589",
  month    =  aug,
  year     =  2021,
  language = "en"
}

@ARTICLE{Xu2021-dt,
  title    = "Variational Circuit Compiler for Quantum Error Correction",
  author   = "Xu, Xiaosi and Benjamin, Simon C and Yuan, Xiao",
  abstract = "Quantum error correction is vital for implementing universal
              quantum computing. A key component is the encoding circuit that
              maps a product state of physical qubits into the encoded
              multipartite entangled logical state. Known methods are typically
              not ``optimal''either in terms of the circuit depth (and
              therefore the error burden) or the specifics of the target
              platform, i.e., the native gates and topology of a system. This
              work introduces a variational compiler for efficiently finding
              the encoding circuit of general quantum error-correcting codes
              with given quantum hardware. Focusing on the noisy
              intermediate-scale quantum regime, we show how to systematically
              compile the circuit following an optimizing process seeking to
              minimize the number of noisy operations that are allowed by the
              noisy quantum hardware or to obtain the highest fidelity of the
              encoded state with noisy gates. We demonstrate our method by
              deriving efficient encoders for logic states of the five-qubit
              code and the seven-qubit Steane code. We describe ways to augment
              the discovered circuits with error detection. Our method is
              applicable quite generally for compiling the encoding circuits of
              quantum error-correcting codes.",
  journal  = "Physical Review Applied",
  volume   =  15,
  number   =  3,
  month    =  nov,
  year     =  2021
}


@article{peruzzo2014variational,
  title={A variational eigenvalue solver on a photonic quantum processor},
  author={Peruzzo, Alberto and McClean, Jarrod and Shadbolt, Peter and Yung, Man-Hong and Zhou, Xiao-Qi and Love, Peter J and Aspuru-Guzik, Al{\'a}n and O’brien, Jeremy L},
  journal={Nature communications},
  volume={5},
  number={1},
  pages={1--7},
  year={2014},
  publisher={Nature Publishing Group}
}

@article{o2016scalable,
  title={Scalable quantum simulation of molecular energies},
  author={O’Malley, Peter JJ and Babbush, Ryan and Kivlichan, Ian D and Romero, Jonathan and McClean, Jarrod R and Barends, Rami and Kelly, Julian and Roushan, Pedram and Tranter, Andrew and Ding, Nan and others},
  journal={Physical Review X},
  volume={6},
  number={3},
  pages={031007},
  year={2016},
  publisher={APS}
}

@inproceedings{colless2017implementing,
  title={Implementing a variational quantum eigensolver using superconducting qubits},
  author={Colless, James and Ramasesh, Vinay and Dahlen, Dar and Blok, Machiel and McClean, Jarrod and Carter, Jonathan and de Jong, Wibe A and Siddiqi, Irfan},
  booktitle={Quantum Information and Measurement},
  pages={QF6A--2},
  year={2017},
  organization={Optical Society of America}
}

@article{kandala2017hardware,
  title={Hardware-efficient variational quantum eigensolver for small molecules and quantum magnets},
  author={Kandala, Abhinav and Mezzacapo, Antonio and Temme, Kristan and Takita, Maika and Brink, Markus and Chow, Jerry M and Gambetta, Jay M},
  journal={Nature},
  volume={549},
  number={7671},
  pages={242--246},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{colless2018computation,
  title={Computation of molecular spectra on a quantum processor with an error-resilient algorithm},
  author={Colless, James I and Ramasesh, Vinay V and Dahlen, Dar and Blok, Machiel S and Kimchi-Schwartz, Mollie E and McClean, Jarrod R and Carter, Jonathan and de Jong, Wibe A and Siddiqi, Irfan},
  journal={Physical Review X},
  volume={8},
  number={1},
  pages={011021},
  year={2018},
  publisher={APS}
}

@article{dumitrescu2018cloud,
  title={Cloud quantum computing of an atomic nucleus},
  author={Dumitrescu, Eugene F and McCaskey, Alex J and Hagen, Gaute and Jansen, Gustav R and Morris, Titus D and Papenbrock, T and Pooser, Raphael C and Dean, David Jarvis and Lougovski, Pavel},
  journal={Physical review letters},
  volume={120},
  number={21},
  pages={210501},
  year={2018},
  publisher={APS}
}





@article{li2017efficient,
  title={Efficient variational quantum simulator incorporating active error minimization},
  author={Li, Ying and Benjamin, Simon C},
  journal={Physical Review X},
  volume={7},
  number={2},
  pages={021050},
  year={2017},
  publisher={APS}
}

@article{mcclean2016theory,
  title={The theory of variational hybrid quantum-classical algorithms},
  author={McClean, Jarrod R and Romero, Jonathan and Babbush, Ryan and Aspuru-Guzik, Al{\'a}n},
  journal={New Journal of Physics},
  volume={18},
  number={2},
  pages={023023},
  year={2016},
  publisher={IOP Publishing}
}

@article{wecker2015progress,
  title={Progress towards practical quantum variational algorithms},
  author={Wecker, Dave and Hastings, Matthew B and Troyer, Matthias},
  journal={Physical Review A},
  volume={92},
  number={4},
  pages={042303},
  year={2015},
  publisher={APS}
}


@ARTICLE{Sun2020-ej,
  title    = "Recent developments in the {PySCF} program package",
  author   = "Sun, Qiming and Zhang, Xing and Banerjee, Samragni and Bao, Peng
              and Barbry, Marc and Blunt, Nick S and Bogdanov, Nikolay A and
              Booth, George H and Chen, Jia and Cui, Zhi-Hao and Eriksen, Janus
              J and Gao, Yang and Guo, Sheng and Hermann, Jan and Hermes,
              Matthew R and Koh, Kevin and Koval, Peter and Lehtola, Susi and
              Li, Zhendong and Liu, Junzi and Mardirossian, Narbe and McClain,
              James D and Motta, Mario and Mussard, Bastien and Pham, Hung Q
              and Pulkin, Artem and Purwanto, Wirawan and Robinson, Paul J and
              Ronca, Enrico and Sayfutyarova, Elvira R and Scheurer, Maximilian
              and Schurkus, Henry F and Smith, James E T and Sun, Chong and
              Sun, Shi-Ning and Upadhyay, Shiv and Wagner, Lucas K and Wang,
              Xiao and White, Alec and Whitfield, James Daniel and Williamson,
              Mark J and Wouters, Sebastian and Yang, Jun and Yu, Jason M and
              Zhu, Tianyu and Berkelbach, Timothy C and Sharma, Sandeep and
              Sokolov, Alexander Yu and Chan, Garnet Kin-Lic",
  abstract = "PySCF is a Python-based general-purpose electronic structure
              platform that supports first-principles simulations of molecules
              and solids as well as accelerates the development of new
              methodology and complex computational workflows. This paper
              explains the design and philosophy behind PySCF that enables it
              to meet these twin objectives. With several case studies, we show
              how users can easily implement their own methods using PySCF as a
              development environment. We then summarize the capabilities of
              PySCF for molecular and solid-state simulations. Finally, we
              describe the growing ecosystem of projects that use PySCF across
              the domains of quantum chemistry, materials science, machine
              learning, and quantum information science.",
  journal  = "J. Chem. Phys.",
  volume   =  153,
  number   =  2,
  pages    = "024109",
  month    =  jul,
  year     =  2020,
  language = "en"
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{Sun2018-nq,
  title     = "{P} y {SCF}: the Python‐based simulations of chemistry framework",
  author    = "Sun, Qiming and Berkelbach, Timothy C and Blunt, Nick S and
               Booth, George H and Guo, Sheng and Li, Zhendong and Liu, Junzi
               and McClain, James D and Sayfutyarova, Elvira R and Sharma,
               Sandeep and Wouters, Sebastian and Chan, Garnet Kin-Lic",
  abstract  = "Python-based simulations of chemistry framework (PySCF) is a
               general-purpose electronic structure platform designed from the
               ground up to emphasize code simplicity, so as to facilitate new
               method development and enable flexible computational workflows.
               The package provides a wide range of tools to support
               simulations of finite-size systems, extended systems with
               periodic boundary conditions, low-dimensional periodic systems,
               and custom Hamiltonians, using mean-field and post-mean-field
               methods with standard Gaussian basis functions. To ensure ease
               of extensibility, PySCF uses the Python language to implement
               almost all of its features, while computationally critical paths
               are implemented with heavily optimized C routines. Using this
               combined Python/C implementation, the package is as efficient as
               the best existing C or Fortran-based quantum chemistry programs.
               In this paper, we document the capabilities and design
               philosophy of the current version of the PySCF package. WIREs
               Comput Mol Sci 2018, 8:e1340. doi: 10.1002/wcms.1340 This
               article is categorized under: Structure and Mechanism >
               Computational Materials Science Electronic Structure Theory > Ab
               Initio Electronic Structure Methods Software > Quantum Chemistry",
  journal   = "Wiley Interdiscip. Rev. Comput. Mol. Sci.",
  publisher = "Wiley",
  volume    =  8,
  number    =  1,
  pages     = "e1340",
  month     =  jan,
  year      =  2018,
  copyright = "http://onlinelibrary.wiley.com/termsAndConditions\#vor",
  language  = "en"
}



@MISC{pennylane_vqls,
  title        = "Variational Quantum Linear Solver --- {PennyLane}",
  abstract     = "Implementing the variational quantum linear solver to solve a
                  system of linear equation with a quantum device.",
  howpublished = "\url{https://pennylane.ai/qml/demos/tutorial_vqls.html}",
  note         = "Accessed: 2022-4-24"
}


@article{HHL,
  title = {Quantum Algorithm for Linear Systems of Equations},
  author = {Harrow, Aram W. and Hassidim, Avinatan and Lloyd, Seth},
  journal = {Phys. Rev. Lett.},
  volume = {103},
  issue = {15},
  pages = {150502},
  numpages = {4},
  year = {2009},
  month = {Oct},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.103.150502},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.103.150502}
}


@misc{Bravo-Prieto_undated-oq,
  doi = {10.48550/ARXIV.1909.05820},
  
  url = {https://arxiv.org/abs/1909.05820},
  
  author = {Bravo-Prieto, Carlos and LaRose, Ryan and Cerezo, M. and Subasi, Yigit and Cincio, Lukasz and Coles, Patrick J.},
  
  keywords = {Quantum Physics (quant-ph), FOS: Physical sciences, FOS: Physical sciences},
  
  title = {Variational Quantum Linear Solver},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



@ARTICLE{Farhi2014-ug,
   title={A quantum approximate optimization algorithm},
  author={Farhi, Edward and Goldstone, Jeffrey and Gutmann, Sam},
  journal={arXiv preprint arXiv:1411.4028},
  year={2014}
}




@ARTICLE{Bharti2022-sw,
  title     = "Noisy intermediate-scale quantum algorithms",
  author    = "Bharti, Kishor and Cervera-Lierta, Alba and Kyaw, Thi Ha and
               Haug, Tobias and Alperin-Lea, Sumner and Anand, Abhinav and
               Degroote, Matthias and Heimonen, Hermanni and Kottmann, Jakob S
               and Menke, Tim and Mok, Wai-Keong and Sim, Sukin and Kwek,
               Leong-Chuan and Aspuru-Guzik, Al{\'a}n",
  journal   = "Rev. Mod. Phys.",
  publisher = "American Physical Society",
  volume    =  94,
  number    =  1,
  pages     = "015004",
  month     =  feb,
  year      =  2022
}



@ARTICLE{9566740mctsqas,
  author={Meng, Fan-Xu and Li, Ze-Tong and Yu, Xu-Tao and Zhang, Zai-Chen},
  journal={IEEE Transactions on Quantum Engineering}, 
  title={Quantum Circuit Architecture Optimization for Variational Quantum Eigensolver via Monto Carlo Tree Search}, 
  year={2021},
  volume={2},
  number={},
  pages={1-10},
  doi={10.1109/TQE.2021.3119010}}

@article{2018preskillnisq,
   title={Quantum Computing in the NISQ era and beyond},
   volume={2},
   ISSN={2521-327X},
   url={http://dx.doi.org/10.22331/q-2018-08-06-79},
   DOI={10.22331/q-2018-08-06-79},
   journal={Quantum},
   publisher={Verein zur Forderung des Open Access Publizierens in den Quantenwissenschaften},
   author={Preskill, John},
   year={2018},
   month={Aug},
   pages={79} }


@book{sakurai_napolitano_2017, 
place={Cambridge}, 
edition={2}, 
title={Modern Quantum Mechanics}, 
DOI={10.1017/9781108499996}, 
publisher={Cambridge University Press}, 
author={Sakurai, J. J. and Napolitano, Jim}, 
year={2017}}

@book{nielsen00,
  added-at = {2010-06-22T17:54:31.000+0200},
  author = {Nielsen, Michael A. and Chuang, Isaac L.},
  biburl = {https://www.bibsonomy.org/bibtex/222bf6f3de23faf420214d738924ac21b/mcclung},
  interhash = {140ce4be72c2994b45286dbaa98d0bd3},
  intrahash = {22bf6f3de23faf420214d738924ac21b},
  keywords = {computing information quantum},
  publisher = {Cambridge University Press},
  timestamp = {2010-06-22T17:54:31.000+0200},
  title = {Quantum Computation and Quantum Information},
  year = 2000
}

@article{2020optionpricing,
   title={Option Pricing using Quantum Computers},
   volume={4},
   ISSN={2521-327X},
   url={http://dx.doi.org/10.22331/q-2020-07-06-291},
   doi={10.22331/q-2020-07-06-291},
   journal={Quantum},
   publisher={Verein zur Forderung des Open Access Publizierens in den Quantenwissenschaften},
   author={Stamatopoulos, Nikitas and Egger, Daniel J. and Sun, Yue and Zoufal, Christa and Iten, Raban and Shen, Ning and Woerner, Stefan},
   year={2020},
   month={Jul},
   pages={291} }

@article{RevModPhys.92.015003,
  title = {Quantum computational chemistry},
  author = {McArdle, Sam and Endo, Suguru and Aspuru-Guzik, Al\'an and Benjamin, Simon C. and Yuan, Xiao},
  journal = {Rev. Mod. Phys.},
  volume = {92},
  issue = {1},
  pages = {015003},
  numpages = {51},
  year = {2020},
  month = {Mar},
  publisher = {American Physical Society},
  doi = {10.1103/RevModPhys.92.015003},
  url = {https://link.aps.org/doi/10.1103/RevModPhys.92.015003}
}

@article{AlphaGoZeroDBLP:journals/nature/SilverSSAHGHBLB17,
  author    = {David Silver and
               Julian Schrittwieser and
               Karen Simonyan and
               Ioannis Antonoglou and
               Aja Huang and
               Arthur Guez and
               Thomas Hubert and
               Lucas Baker and
               Matthew Lai and
               Adrian Bolton and
               Yutian Chen and
               Timothy P. Lillicrap and
               Fan Hui and
               Laurent Sifre and
               George van den Driessche and
               Thore Graepel and
               Demis Hassabis},
  title     = {Mastering the game of Go without human knowledge},
  journal   = {Nat.},
  volume    = {550},
  number    = {7676},
  pages     = {354--359},
  year      = {2017},
  url       = {https://doi.org/10.1038/nature24270},
  doi       = {10.1038/nature24270},
  timestamp = {Mon, 27 Sep 2021 17:38:55 +0200},
  biburl    = {https://dblp.org/rec/journals/nature/SilverSSAHGHBLB17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{AlphaGoDBLP:journals/nature/SilverHMGSDSAPL16,
  author    = {David Silver and
               Aja Huang and
               Chris J. Maddison and
               Arthur Guez and
               Laurent Sifre and
               George van den Driessche and
               Julian Schrittwieser and
               Ioannis Antonoglou and
               Vedavyas Panneershelvam and
               Marc Lanctot and
               Sander Dieleman and
               Dominik Grewe and
               John Nham and
               Nal Kalchbrenner and
               Ilya Sutskever and
               Timothy P. Lillicrap and
               Madeleine Leach and
               Koray Kavukcuoglu and
               Thore Graepel and
               Demis Hassabis},
  title     = {Mastering the game of Go with deep neural networks and tree search},
  journal   = {Nat.},
  volume    = {529},
  number    = {7587},
  pages     = {484--489},
  year      = {2016},
  url       = {https://doi.org/10.1038/nature16961},
  doi       = {10.1038/nature16961},
  timestamp = {Mon, 27 Sep 2021 17:38:59 +0200},
  biburl    = {https://dblp.org/rec/journals/nature/SilverHMGSDSAPL16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{ENASpmlr-v80-pham18a,
  title = 	 {Efficient Neural Architecture Search via Parameters Sharing},
  author =       {Pham, Hieu and Guan, Melody and Zoph, Barret and Le, Quoc and Dean, Jeff},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {4095--4104},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/pham18a/pham18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/pham18a.html},
  abstract = 	 {We propose Efficient Neural Architecture Search (ENAS), a fast and inexpensive approach for automatic model design. ENAS constructs a large computational graph, where each subgraph represents a neural network architecture, hence forcing all architectures to share their parameters. A controller is trained with policy gradient to search for a subgraph that maximizes the expected reward on a validation set. Meanwhile a model corresponding to the selected subgraph is trained to minimize a canonical cross entropy loss. Sharing parameters among child models allows ENAS to deliver strong empirical performances, whilst using much fewer GPU-hours than existing automatic model design approaches, and notably, 1000x less expensive than standard Neural Architecture Search. On Penn Treebank, ENAS discovers a novel architecture that achieves a test perplexity of 56.3, on par with the existing state-of-the-art among all methods without post-training processing. On CIFAR-10, ENAS finds a novel architecture that achieves 2.89% test error, which is on par with the 2.65% test error of NASNet (Zoph et al., 2018).}
}


@article{PDARTSDBLP:journals/corr/abs-1904-12760,
  author    = {Xin Chen and
               Lingxi Xie and
               Jun Wu and
               Qi Tian},
  title     = {Progressive Differentiable Architecture Search: Bridging the Depth
               Gap between Search and Evaluation},
  journal   = {CoRR},
  volume    = {abs/1904.12760},
  year      = {2019},
  url       = {http://arxiv.org/abs/1904.12760},
  eprinttype = {arXiv},
  eprint    = {1904.12760},
  timestamp = {Fri, 30 Apr 2021 09:23:03 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1904-12760.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{AlphaXDBLP:conf/aaai/WangZJTF20,
  author    = {Linnan Wang and
               Yiyang Zhao and
               Yuu Jinnai and
               Yuandong Tian and
               Rodrigo Fonseca},
  title     = {Neural Architecture Search Using Deep Neural Networks and Monte Carlo
               Tree Search},
  booktitle = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
               2020, The Thirty-Second Innovative Applications of Artificial Intelligence
               Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
               Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
               February 7-12, 2020},
  pages     = {9983--9991},
  publisher = {{AAAI} Press},
  year      = {2020},
  url       = {https://aaai.org/ojs/index.php/AAAI/article/view/6554},
  timestamp = {Tue, 02 Feb 2021 08:00:57 +0100},
  biburl    = {https://dblp.org/rec/conf/aaai/WangZJTF20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{PNAS10.1007/978-3-030-01246-5_2,
author="Liu, Chenxi
and Zoph, Barret
and Neumann, Maxim
and Shlens, Jonathon
and Hua, Wei
and Li, Li-Jia
and Fei-Fei, Li
and Yuille, Alan
and Huang, Jonathan
and Murphy, Kevin",
editor="Ferrari, Vittorio
and Hebert, Martial
and Sminchisescu, Cristian
and Weiss, Yair",
title="Progressive Neural Architecture Search",
booktitle="Computer Vision -- ECCV 2018",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="19--35",
abstract="We propose a new method for learning the structure of convolutional neural networks (CNNs) that is more efficient than recent state-of-the-art methods based on reinforcement learning and evolutionary algorithms. Our approach uses a sequential model-based optimization (SMBO) strategy, in which we search for structures in order of increasing complexity, while simultaneously learning a surrogate model to guide the search through structure space. Direct comparison under the same search space shows that our method is up to 5 times more efficient than the RL method of Zoph et al. (2018) in terms of number of models evaluated, and 8 times faster in terms of total compute. The structures we discover in this way achieve state of the art classification accuracies on CIFAR-10 and ImageNet.",
isbn="978-3-030-01246-5"
}


@book{schuldpetruccione2021, 
place={Cham, Switzerland}, 
title={Machine learning with Quantum Computers}, 
publisher={Springer}, 
author={Schuld, Maria and Petruccione, Francesco}, 
year={2021}} 

@misc{johnson2017qvector,
      title={QVECTOR: an algorithm for device-tailored quantum error correction}, 
      author={Peter D. Johnson and Jonathan Romero and Jonathan Olson and Yudong Cao and Alán Aspuru-Guzik},
      year={2017},
      eprint={1711.02249},
      archivePrefix={arXiv},
      primaryClass={quant-ph}
}

@article{2017hardwareefficientvqe,
   title={Hardware-efficient variational quantum eigensolver for small molecules and quantum magnets},
   volume={549},
   ISSN={1476-4687},
   url={http://dx.doi.org/10.1038/nature23879},
   DOI={10.1038/nature23879},
   number={7671},
   journal={Nature},
   publisher={Springer Science and Business Media LLC},
   author={Kandala, Abhinav and Mezzacapo, Antonio and Temme, Kristan and Takita, Maika and Brink, Markus and Chow, Jerry M. and Gambetta, Jay M.},
   year={2017},
   month={Sep},
   pages={242–246}
}

@article{physicalinspiredansatze1doi:10.1021/acs.jctc.8b01004,
author = {Lee, Joonho and Huggins, William J. and Head-Gordon, Martin and Whaley, K. Birgitta},
title = {Generalized Unitary Coupled Cluster Wave functions for Quantum Computation},
journal = {Journal of Chemical Theory and Computation},
volume = {15},
number = {1},
pages = {311-324},
year = {2019},
doi = {10.1021/acs.jctc.8b01004},
}

@inproceedings{DARTS_DBLP:conf/iclr/LiuSY19,
  author    = {Hanxiao Liu and
               Karen Simonyan and
               Yiming Yang},
  title     = {{DARTS:} Differentiable Architecture Search},
  booktitle = {7th International Conference on Learning Representations, {ICLR} 2019,
               New Orleans, LA, USA, May 6-9, 2019},
  publisher = {OpenReview.net},
  year      = {2019},
  url       = {https://openreview.net/forum?id=S1eYHoC5FX},
  timestamp = {Thu, 25 Jul 2019 14:25:55 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/LiuSY19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{zhang2021differentiable,
      title={Differentiable Quantum Architecture Search}, 
      author={Shi-Xin Zhang and Chang-Yu Hsieh and Shengyu Zhang and Hong Yao},
      year={2021},
      eprint={2010.08561},
      archivePrefix={arXiv},
      primaryClass={quant-ph}
}


@ARTICLE{du2020quantum,
  title     = "Quantum circuit architecture search for variational quantum
               algorithms",
  author    = "Du, Yuxuan and Huang, Tao and You, Shan and Hsieh, Min-Hsiu and
               Tao, Dacheng",
  abstract  = "Variational quantum algorithms (VQAs) are expected to be a path
               to quantum advantages on noisy intermediate-scale quantum
               devices. However, both empirical and theoretical results exhibit
               that the deployed ansatz heavily affects the performance of VQAs
               such that an ansatz with a larger number of quantum gates
               enables a stronger expressivity, while the accumulated noise may
               render a poor trainability. To maximally improve the robustness
               and trainability of VQAs, here we devise a resource and runtime
               efficient scheme termed quantum architecture search (QAS). In
               particular, given a learning task, QAS automatically seeks a
               near-optimal ansatz (i.e., circuit architecture) to balance
               benefits and side-effects brought by adding more noisy quantum
               gates to achieve a good performance. We implement QAS on both
               the numerical simulator and real quantum hardware, via the IBM
               cloud, to accomplish data classification and quantum chemistry
               tasks. In the problems studied, numerical and experimental
               results show that QAS cannot only alleviate the influence of
               quantum noise and barren plateaus but also outperforms VQAs with
               pre-selected ansatze.",
  journal   = "npj Quantum Information",
  publisher = "Nature Publishing Group",
  volume    =  8,
  number    =  1,
  pages     = "1--8",
  month     =  may,
  year      =  2022,
  language  = "en"
}



@article{zhang2021neural,
	doi = {10.1088/2632-2153/ac28dd},
	url = {https://doi.org/10.1088/2632-2153/ac28dd},
	year = 2021,
	month = {Oct},
	publisher = {{IOP} Publishing},
	volume = {2},
	number = {4},
	pages = {045027},
	author = {Shi-Xin Zhang and Chang-Yu Hsieh and Shengyu Zhang and Hong Yao},
	title = {Neural predictor based quantum architecture search},
	journal = {Machine Learning: Science and Technology}
}

@misc{kuo2021quantum,
      title={Quantum Architecture Search via Deep Reinforcement Learning}, 
      author={En-Jui Kuo and Yao-Lung L. Fang and Samuel Yen-Chi Chen},
      year={2021},
      eprint={2104.07715},
      archivePrefix={arXiv},
      primaryClass={quant-ph}
}

% The entry below contains non-ASCII chars that could not be converted
% to a LaTeX equivalent.
@ARTICLE{chen2021quantum,
  title     = "Quantum architecture search with meta‐learning",
  author    = "He, Zhimin and Chen, Chuangtao and Li, Lvzhou and Zheng,
               Shenggen and Situ, Haozhen",
  abstract  = "Abstract Variational quantum algorithms (VQAs) have been
               successfully applied to quantum approximate optimization
               algorithms, variational quantum compiling and quantum machine
               learning models. The performances of VQAs largely depend on the
               architecture of parameterized quantum circuits (PQCs). Quantum
               architecture search (QAS) aims to automate the design of PQCs in
               different VQAs with classical optimization algorithms. However,
               current QAS algorithms do not use prior experiences and search
               the quantum architecture from scratch for each new task, which
               is inefficient and time consuming. In this paper, a meta quantum
               architecture search (MetaQAS) algorithm is proposed, which
               learns good initialization heuristics of the architecture (i.e.,
               meta-architecture), along with the meta-parameters of quantum
               gates from a number of training tasks such that they can adapt
               to new tasks with fewer gradient updates, which leads to fast
               learning on new tasks. The proposed MetaQAS can be used with
               arbitrary gradient-based QAS algorithms. Simulation results on
               variational quantum compiling (VQC) and quantum approximate
               optimization algorithm (QAOA) show that the architectures
               optimized by MetaQAS converge faster than a state-of-the-art
               gradient-based QAS algorithm, namely DQAS. MetaQAS also achieves
               a better solution than DQAS after fine-tuning of gate
               parameters.",
  journal   = "Adv. Quantum Technol.",
  publisher = "Wiley",
  pages     = "2100134",
  month     =  jun,
  year      =  2022,
  copyright = "http://onlinelibrary.wiley.com/termsAndConditions\#vor",
  language  = "en"
}



@inproceedings{MCTS_for_game10.5555/3022539.3022579,
author = {Chaslot, Guillaume and Bakkes, Sander and Szita, Istvan and Spronck, Pieter},
title = {Monte-Carlo Tree Search: A New Framework for Game AI},
year = {2008},
publisher = {AAAI Press},
booktitle = {Proceedings of the Fourth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
pages = {216–217},
numpages = {2},
location = {Stanford, California},
series = {AIIDE'08}
}

@article{UCB_paper_10.5555/944919.944941,
author = {Auer, Peter},
title = {Using Confidence Bounds for Exploitation-Exploration Trade-Offs},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
month = {Mar},
pages = {397–422},
numpages = {26},
keywords = {online Learning, exploitation-exploration, reinforcement learning, linear value function, bandit problem}
}



@inproceedings{nestedmontecarlosearch,
    author = {Cazenave, Tristan},
    title = {Nested Monte-Carlo Search},
    year = {2009},
    publisher = {Morgan Kaufmann Publishers Inc.},
    address = {San Francisco, CA, USA},
    booktitle = {Proceedings of the 21st International Joint Conference on Artificial Intelligence},
    pages = {456–461},
    numpages = {6},
    location = {Pasadena, California, USA},
    series = {IJCAI'09},
    doi = {10.5555/1661445.1661518}
}

@inproceedings{CMAB_RTS,
    author = {Onta\~{n}\'{o}n, Santiago},
    title = {The Combinatorial Multi-Armed Bandit Problem and Its Application to Real-Time Strategy Games},
    year = {2013},
    isbn = {1577356071},
    publisher = {AAAI Press},
    abstract = {Game tree search in games with large branching factors is a notoriously hard problem. In this paper, we address this problem with a new sampling strategy for Monte Carlo Tree Search (MCTS) algorithms, called Na\"{\i}ve Sampling, based on a variant of the Multi-armed Bandit problem called the Combinatorial Multi-armed Bandit (CMAB) problem. We present a new MCTS algorithm based on Na\"{\i}ve Sampling called Na\"{\i}veMCTS, and evaluate it in the context of real-time strategy (RTS) games. Our results show that as the branching factor grows, Na\"{\i}veMCTS performs significantly better than other algorithms.},
    booktitle = {Proceedings of the Ninth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment},
    pages = {58–64},
    numpages = {7},
    location = {Boston, MA, USA},
    series = {AIIDE'13},
    doi={10.5555/3014712.3014722}
}

@article{qec_intro_guide,
    author = {Joschka Roffe},
    title = {Quantum error correction: an introductory guide},
    journal = {Contemporary Physics},
    volume = {60},
    number = {3},
    pages = {226-245},
    year  = {2019},
    publisher = {Taylor \& Francis},
    doi = {10.1080/00107514.2019.1667078}
}

@inproceedings{huang2021neural,
  author    = {Hanxun Huang and
               Xingjun Ma and
               Sarah M. Erfani and
               James Bailey},
  title     = {Neural Architecture Search via Combinatorial Multi-Armed Bandit},
  booktitle = {International Joint Conference on Neural Networks, {IJCNN} 2021, Shenzhen,
               China, July 18-22, 2021},
  pages     = {1--8},
  publisher = {{IEEE}},
  year      = {2021},
  url       = {https://doi.org/10.1109/IJCNN52387.2021.9533655},
  doi       = {10.1109/IJCNN52387.2021.9533655},
  timestamp = {Wed, 29 Sep 2021 17:01:12 +0200},
  biburl    = {https://dblp.org/rec/conf/ijcnn/HuangMEB21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{bergholm2020pennylane,
      title={PennyLane: Automatic differentiation of hybrid quantum-classical computations}, 
      author={Ville Bergholm and Josh Izaac and Maria Schuld and Christian Gogolin and M. Sohaib Alam and Shahnawaz Ahmed and Juan Miguel Arrazola and Carsten Blank and Alain Delgado and Soran Jahangiri and Keri McKiernan and Johannes Jakob Meyer and Zeyue Niu and Antal Száva and Nathan Killoran},
      year={2020},
      eprint={1811.04968},
      archivePrefix={arXiv},
      primaryClass={quant-ph}
}

@misc{pennylane_dev_team_2021, 
    title={A brief overview of VQE}, 
    url={https://pennylane.ai/qml/demos/tutorial_vqe.html}, 
    journal={PennyLane}, 
    publisher={Xanadu Quantum Technologies Inc.}, 
    author={PennyLane dev team}, 
    year={2021}, 
    month={Jul}
} 