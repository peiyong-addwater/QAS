{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Research\\QAS-Qiskit\\QAS\n",
      "dict_keys(['task', 'pool', 'params', 'k', 'op_list', 'search_reward_list', 'fine_tune_loss'])\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml \n",
    "import pennylane.numpy as pnp \n",
    "import numpy as np \n",
    "from qas import qml_gate_ops \n",
    "from qas import qml_models \n",
    "from qas import mcts\n",
    "import os \n",
    "import json \n",
    "from pprint import pprint\n",
    "cwd = os.getcwd() \n",
    "print(cwd)\n",
    "\n",
    "res_file = '20211009-231603.json'\n",
    "with open(os.path.join(cwd, res_file)) as f:\n",
    "    res_dict = json.load(f)\n",
    "\n",
    "print(res_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 12, 3)\n",
      "[11, 8, 11, 8, 2, 3, 0, 1, 4, 5, 1, 3, 5, 5, 1]\n",
      "{0: {'Rot': [0]}, 1: {'PlaceHolder': [0]}, 2: {'Rot': [1]}, 3: {'PlaceHolder': [1]}, 4: {'Rot': [2]}, 5: {'PlaceHolder': [2]}, 6: {'CRot': [0, 1]}, 7: {'CNOT': [0, 1]}, 8: {'CRot': [1, 2]}, 9: {'CNOT': [1, 2]}, 10: {'CRot': [0, 2]}, 11: {'CNOT': [0, 2]}}\n",
      "[('CNOT', [0, 2], None), ('CRot', [1, 2], [1.129680337296492, 1.5707922440908313, -0.0008633026512832973]), ('CNOT', [0, 2], None), ('CRot', [1, 2], [0.026147067452042146, -1.26553346484485, 0.8799594504776643]), ('Rot', [1], [-0.7463000265359258, 0.000728433748695468, 0.7427270846724496]), ('PlaceHolder', [1], None), ('Rot', [0], [-0.7774424770311453, 0.0006764088145414327, -0.3979643430853004]), ('PlaceHolder', [0], None), ('Rot', [2], [-0.942913291994291, -0.1571363759045717, 0.6860885220354251]), ('PlaceHolder', [2], None), ('PlaceHolder', [0], None), ('PlaceHolder', [1], None), ('PlaceHolder', [2], None), ('PlaceHolder', [2], None), ('PlaceHolder', [0], None)]\n"
     ]
    }
   ],
   "source": [
    "pool = {int(k):res_dict['pool'][k] for k in res_dict['pool'].keys()}\n",
    "params = np.array(res_dict['params'])\n",
    "print(params.shape)\n",
    "print(res_dict['k'])\n",
    "print(pool)\n",
    "model = qml_models.ToffoliQMLNoiseless(params.shape[0], params.shape[1], params.shape[2], res_dict['k'], pool)\n",
    "print(model.toList(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = qml.device('default.qubit', wires = 3)\n",
    "@qml.qnode(dev)\n",
    "def searched_circ(x):\n",
    "    qml.QubitStateVector(x, wires=[0,1,2])\n",
    "    qml.CNOT(wires =[0,2])\n",
    "    qml.CRot(1.129680337296492, 1.5707922440908313, -0.0008633026512832973, wires=[1,2])\n",
    "    qml.CNOT(wires =[0,2])\n",
    "    qml.CRot(0.026147067452042146, -1.26553346484485, 0.8799594504776643, wires=[1,2])\n",
    "    qml.Rot(-0.7463000265359258, 0.000728433748695468, 0.7427270846724496, wires = 1)\n",
    "    qml.Rot(-0.7774424770311453, 0.0006764088145414327, -0.3979643430853004, wires = 0)\n",
    "    qml.Rot(-0.942913291994291, -0.1571363759045717, 0.6860885220354251, wires = 2)\n",
    "    return qml.state()\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def target_circ(x):\n",
    "    qml.QubitStateVector(x, wires=[0,1,2])\n",
    "    qml.Toffoli(wires=[0,1,2])\n",
    "    return qml.state()\n",
    "\n",
    "computational_bases = {\"0\":np.array([1,0]), \"1\":np.array([0,1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0: ──╭QubitStateVector(M0)──╭C───────────────────────────────╭C───Rot(-0.777, 0.000676, -0.398)────────────────────────────────╭┤ State \n",
      " 1: ──├QubitStateVector(M0)──│───╭C───────────────────────────│───╭C──────────────────────────────Rot(-0.746, 0.000728, 0.743)──├┤ State \n",
      " 2: ──╰QubitStateVector(M0)──╰X──╰Rot(1.13, 1.57, -0.000863)──╰X──╰Rot(0.0261, -1.27, 0.88)───────Rot(-0.943, -0.157, 0.686)────╰┤ State \n",
      "M0 =\n",
      "[0 0 0 0 0 0 1 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(qml.draw(searched_circ)(np.kron(computational_bases['1'], np.kron(computational_bases['1'], computational_bases['0']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State:  000\n",
      "State Vector: \n",
      "[1 0 0 0 0 0 0 0]\n",
      "Searched Circ Output:\n",
      "[ 7.5086e-01+6.5578e-01j -1.3031e-02-7.7398e-02j  3.9920e-05+3.6089e-04j  1.5568e-05-2.3975e-05j  3.2005e-04+1.0604e-04j -1.4207e-05-2.2423e-05j  5.9748e-08+1.0728e-07j  1.7114e-09-9.5154e-09j]\n",
      "Target Circ Output:\n",
      "[1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      "\n",
      "State:  001\n",
      "State Vector: \n",
      "[0 1 0 0 0 0 0 0]\n",
      "Searched Circ Output:\n",
      "[7.6509e-02-1.7512e-02j 8.9281e-01+4.4354e-01j 2.4840e-05+1.4147e-05j 1.3028e-04+3.3892e-04j 2.1558e-05-1.5488e-05j 3.3649e-04+2.1265e-05j 9.5988e-09+1.1550e-09j 8.5039e-08+8.8589e-08j]\n",
      "Target Circ Output:\n",
      "[0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      "\n",
      "State:  010\n",
      "State Vector: \n",
      "[0 0 1 0 0 0 0 0]\n",
      "Searched Circ Output:\n",
      "[-1.8545e-04+3.1229e-04j -2.5883e-05+8.1837e-06j  9.5495e-01-2.8724e-01j  6.7545e-02+3.1510e-02j -1.6888e-08+1.2167e-07j -6.9971e-09+5.9440e-09j  2.6008e-04-2.1472e-04j  2.5189e-05+9.7106e-07j]\n",
      "Target Circ Output:\n",
      "[0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      "\n",
      "State:  011\n",
      "State Vector: \n",
      "[0 0 0 1 0 0 0 0]\n",
      "Searched Circ Output:\n",
      "[ 2.7146e-05-2.0060e-07j -2.7275e-04-2.3984e-04j -5.5274e-02-5.0000e-02j  1.0628e-01+9.9154e-01j  8.4370e-09-3.6205e-09j -1.1647e-07-3.9028e-08j -2.3786e-05-8.3439e-06j  1.6309e-04+2.9521e-04j]\n",
      "Target Circ Output:\n",
      "[0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      "\n",
      "State:  100\n",
      "State Vector: \n",
      "[0 0 0 0 1 0 0 0]\n",
      "Searched Circ Output:\n",
      "[-3.3656e-04+2.0063e-05j  2.1503e-05+1.5565e-05j -9.5239e-08-7.7520e-08j  1.9352e-09+9.4724e-09j  8.9439e-01-4.4035e-01j -7.6446e-02-1.7785e-02j  3.4842e-04+1.0216e-04j -1.6129e-05-2.3602e-05j]\n",
      "Target Circ Output:\n",
      "[0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      "\n",
      "State:  101\n",
      "State Vector: \n",
      "[0 0 0 0 0 1 0 0]\n",
      "Searched Circ Output:\n",
      "[-1.4287e-05+2.2372e-05j -3.2043e-04+1.0490e-04j -9.3438e-09+2.4829e-09j -1.1181e-07-5.0785e-08j  1.3308e-02-7.7351e-02j  7.5320e-01-6.5309e-01j  2.2623e-05-1.7475e-05j  3.6295e-04+1.0310e-05j]\n",
      "Target Circ Output:\n",
      "[0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j]\n",
      "\n",
      "State:  110\n",
      "State Vector: \n",
      "[0 0 0 0 0 0 1 0]\n",
      "Searched Circ Output:\n",
      "[ 6.4291e-09-6.5709e-09j -1.2264e-07+6.8860e-09j -2.5204e-05+1.3516e-06j  2.6083e-04+2.1380e-04j -9.9945e-06+2.5277e-05j  3.2640e-04-1.5930e-04j  6.7150e-02-3.2565e-02j -9.5595e-01-2.8389e-01j]\n",
      "Target Circ Output:\n",
      "[0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j]\n",
      "\n",
      "State:  111\n",
      "State Vector: \n",
      "[0 0 0 0 0 0 0 1]\n",
      "Searched Circ Output:\n",
      "[-2.9392e-08-1.1927e-07j  4.3960e-09-8.0737e-09j -1.6202e-04+2.9580e-04j -2.3883e-05+8.1664e-06j  2.1678e-04+2.9141e-04j -2.7306e-06+2.7044e-05j  1.0267e-01-9.9191e-01j  5.5740e-02-4.9626e-02j]\n",
      "Target Circ Output:\n",
      "[0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for a in computational_bases.keys():\n",
    "    for b in computational_bases.keys():\n",
    "        for c in computational_bases.keys():\n",
    "            state_name = a+b+c\n",
    "            state = np.kron(np.kron(computational_bases[a], computational_bases[b]), computational_bases[c])\n",
    "            print(\"State: \", state_name)\n",
    "            print(\"State Vector: \")\n",
    "            print(state)\n",
    "            print(\"Searched Circ Output:\")\n",
    "            print(searched_circ(state))\n",
    "            print(\"Target Circ Output:\")\n",
    "            print(target_circ(state))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Circuit at Epoch 1/400; Loss: 0.005861596671831282\n",
      "Training Circuit at Epoch 2/400; Loss: 0.005524210911090011\n",
      "Training Circuit at Epoch 3/400; Loss: 0.005233116530757975\n",
      "Training Circuit at Epoch 4/400; Loss: 0.0049940789727001\n",
      "Training Circuit at Epoch 5/400; Loss: 0.004806161081916116\n",
      "Training Circuit at Epoch 6/400; Loss: 0.004641285590759714\n",
      "Training Circuit at Epoch 7/400; Loss: 0.004473184371652006\n",
      "Training Circuit at Epoch 8/400; Loss: 0.004298579835388505\n",
      "Training Circuit at Epoch 9/400; Loss: 0.004128832752743783\n",
      "Training Circuit at Epoch 10/400; Loss: 0.00397208132046778\n",
      "Training Circuit at Epoch 11/400; Loss: 0.0038277220028939407\n",
      "Training Circuit at Epoch 12/400; Loss: 0.003689204113827005\n",
      "Training Circuit at Epoch 13/400; Loss: 0.003551246209106962\n",
      "Training Circuit at Epoch 14/400; Loss: 0.003414624114720377\n",
      "Training Circuit at Epoch 15/400; Loss: 0.003283625620733255\n",
      "Training Circuit at Epoch 16/400; Loss: 0.003161212888130871\n",
      "Training Circuit at Epoch 17/400; Loss: 0.0030466832807664046\n",
      "Training Circuit at Epoch 18/400; Loss: 0.002936632240268411\n",
      "Training Circuit at Epoch 19/400; Loss: 0.002828111405883016\n",
      "Training Circuit at Epoch 20/400; Loss: 0.002720783343849842\n",
      "Training Circuit at Epoch 21/400; Loss: 0.0026160842099222092\n",
      "Training Circuit at Epoch 22/400; Loss: 0.0025150428315846574\n",
      "Training Circuit at Epoch 23/400; Loss: 0.0024170639175735698\n",
      "Training Circuit at Epoch 24/400; Loss: 0.0023205292130299426\n",
      "Training Circuit at Epoch 25/400; Loss: 0.002224496648538965\n",
      "Training Circuit at Epoch 26/400; Loss: 0.0021296464557016836\n",
      "Training Circuit at Epoch 27/400; Loss: 0.002037537214223506\n",
      "Training Circuit at Epoch 28/400; Loss: 0.00194915919424421\n",
      "Training Circuit at Epoch 29/400; Loss: 0.0018642269614184137\n",
      "Training Circuit at Epoch 30/400; Loss: 0.0017817453783255388\n",
      "Training Circuit at Epoch 31/400; Loss: 0.0017011440567689107\n",
      "Training Circuit at Epoch 32/400; Loss: 0.0016227429026153262\n",
      "Training Circuit at Epoch 33/400; Loss: 0.0015471929607352441\n",
      "Training Circuit at Epoch 34/400; Loss: 0.001474668302465787\n",
      "Training Circuit at Epoch 35/400; Loss: 0.0014046801269105291\n",
      "Training Circuit at Epoch 36/400; Loss: 0.0013366193150265993\n",
      "Training Circuit at Epoch 37/400; Loss: 0.0012703384211748547\n",
      "Training Circuit at Epoch 38/400; Loss: 0.0012061292950769076\n",
      "Training Circuit at Epoch 39/400; Loss: 0.0011442416580617065\n",
      "Training Circuit at Epoch 40/400; Loss: 0.0010845695007026812\n",
      "Training Circuit at Epoch 41/400; Loss: 0.0010268380409974576\n",
      "Training Circuit at Epoch 42/400; Loss: 0.0009709933318122754\n",
      "Training Circuit at Epoch 43/400; Loss: 0.0009172795930680877\n",
      "Training Circuit at Epoch 44/400; Loss: 0.0008659347458191968\n",
      "Training Circuit at Epoch 45/400; Loss: 0.0008169094396492937\n",
      "Training Circuit at Epoch 46/400; Loss: 0.0007699403827381435\n",
      "Training Circuit at Epoch 47/400; Loss: 0.0007248472178149612\n",
      "Training Circuit at Epoch 48/400; Loss: 0.0006816661249389178\n",
      "Training Circuit at Epoch 49/400; Loss: 0.0006404904604602191\n",
      "Training Circuit at Epoch 50/400; Loss: 0.0006012689445731301\n",
      "Training Circuit at Epoch 51/400; Loss: 0.0005638208855710536\n",
      "Training Circuit at Epoch 52/400; Loss: 0.0005280126476456815\n",
      "Training Circuit at Epoch 53/400; Loss: 0.0004938424190854551\n",
      "Training Circuit at Epoch 54/400; Loss: 0.00046134539920883455\n",
      "Training Circuit at Epoch 55/400; Loss: 0.0004304826354404323\n",
      "Training Circuit at Epoch 56/400; Loss: 0.0004011651209326894\n",
      "Training Circuit at Epoch 57/400; Loss: 0.00037334960492985125\n",
      "Training Circuit at Epoch 58/400; Loss: 0.00034704988110045853\n",
      "Training Circuit at Epoch 59/400; Loss: 0.0003222537879763454\n",
      "Training Circuit at Epoch 60/400; Loss: 0.0002988774449124154\n",
      "Training Circuit at Epoch 61/400; Loss: 0.0002768205757721631\n",
      "Training Circuit at Epoch 62/400; Loss: 0.0002560331827758944\n",
      "Training Circuit at Epoch 63/400; Loss: 0.00023649858195573525\n",
      "Training Circuit at Epoch 64/400; Loss: 0.00021817196912454762\n",
      "Training Circuit at Epoch 65/400; Loss: 0.00020097134132490702\n",
      "Training Circuit at Epoch 66/400; Loss: 0.00018482552905152438\n",
      "Training Circuit at Epoch 67/400; Loss: 0.00016969984311532293\n",
      "Training Circuit at Epoch 68/400; Loss: 0.00015556782666326097\n",
      "Training Circuit at Epoch 69/400; Loss: 0.00014238353901518952\n",
      "Training Circuit at Epoch 70/400; Loss: 0.0001300951767522207\n",
      "Training Circuit at Epoch 71/400; Loss: 0.00011866677999683439\n",
      "Training Circuit at Epoch 72/400; Loss: 0.00010806794746720527\n",
      "Training Circuit at Epoch 73/400; Loss: 9.825256806450522e-05\n",
      "Training Circuit at Epoch 74/400; Loss: 8.916510401602018e-05\n",
      "Training Circuit at Epoch 75/400; Loss: 8.076252005650986e-05\n",
      "Training Circuit at Epoch 76/400; Loss: 7.301429870465448e-05\n",
      "Training Circuit at Epoch 77/400; Loss: 6.588350695357903e-05\n",
      "Training Circuit at Epoch 78/400; Loss: 5.932315832479773e-05\n",
      "Training Circuit at Epoch 79/400; Loss: 5.329227961081706e-05\n",
      "Training Circuit at Epoch 80/400; Loss: 4.776277417317676e-05\n",
      "Training Circuit at Epoch 81/400; Loss: 4.270793678307072e-05\n",
      "Training Circuit at Epoch 82/400; Loss: 3.809519243680448e-05\n",
      "Training Circuit at Epoch 83/400; Loss: 3.389347561821854e-05\n",
      "Training Circuit at Epoch 84/400; Loss: 3.007802780508051e-05\n",
      "Training Circuit at Epoch 85/400; Loss: 2.6623494830957384e-05\n",
      "Training Circuit at Epoch 86/400; Loss: 2.3499951034122724e-05\n",
      "Training Circuit at Epoch 87/400; Loss: 2.0679758798158865e-05\n",
      "Training Circuit at Epoch 88/400; Loss: 1.8141781883418417e-05\n",
      "Training Circuit at Epoch 89/400; Loss: 1.586530191077795e-05\n",
      "Training Circuit at Epoch 90/400; Loss: 1.3825944938705703e-05\n",
      "Training Circuit at Epoch 91/400; Loss: 1.2001198321898165e-05\n",
      "Training Circuit at Epoch 92/400; Loss: 1.0374634441201458e-05\n",
      "Training Circuit at Epoch 93/400; Loss: 8.931502793352308e-06\n",
      "Training Circuit at Epoch 94/400; Loss: 7.654847498761441e-06\n",
      "Training Circuit at Epoch 95/400; Loss: 6.528572919162201e-06\n",
      "Training Circuit at Epoch 96/400; Loss: 5.539877625859724e-06\n",
      "Training Circuit at Epoch 97/400; Loss: 4.676265195291407e-06\n",
      "Training Circuit at Epoch 98/400; Loss: 3.923927879623257e-06\n",
      "Training Circuit at Epoch 99/400; Loss: 3.2707229264961057e-06\n",
      "Training Circuit at Epoch 100/400; Loss: 2.707228892839808e-06\n",
      "Training Circuit at Epoch 101/400; Loss: 2.2238362913373777e-06\n",
      "Training Circuit at Epoch 102/400; Loss: 1.8101333617570603e-06\n",
      "Training Circuit at Epoch 103/400; Loss: 1.4577283373728989e-06\n",
      "Training Circuit at Epoch 104/400; Loss: 1.1606000900332702e-06\n",
      "Training Circuit at Epoch 105/400; Loss: 9.124839935870099e-07\n",
      "Training Circuit at Epoch 106/400; Loss: 7.065581524301479e-07\n",
      "Training Circuit at Epoch 107/400; Loss: 5.373486523518878e-07\n",
      "Training Circuit at Epoch 108/400; Loss: 4.0052527383593883e-07\n",
      "Training Circuit at Epoch 109/400; Loss: 2.9136821000363966e-07\n",
      "Training Circuit at Epoch 110/400; Loss: 2.052830219145818e-07\n",
      "Training Circuit at Epoch 111/400; Loss: 1.3890571937125173e-07\n",
      "Training Circuit at Epoch 112/400; Loss: 8.929537087531969e-08\n",
      "Training Circuit at Epoch 113/400; Loss: 5.317051210429469e-08\n",
      "Training Circuit at Epoch 114/400; Loss: 2.7867625540523022e-08\n",
      "Training Circuit at Epoch 115/400; Loss: 1.1773312769136624e-08\n",
      "Training Circuit at Epoch 116/400; Loss: 3.2671961802677174e-09\n",
      "Training Circuit at Epoch 117/400; Loss: 4.3366610302797426e-10\n",
      "Training Circuit at Epoch 118/400; Loss: 1.8889162456403596e-09\n",
      "Training Circuit at Epoch 119/400; Loss: 6.747171932275364e-09\n",
      "Training Circuit at Epoch 120/400; Loss: 1.394242121754985e-08\n",
      "Training Circuit at Epoch 121/400; Loss: 2.2433649382591625e-08\n",
      "Training Circuit at Epoch 122/400; Loss: 3.163386397275758e-08\n",
      "Training Circuit at Epoch 123/400; Loss: 4.105968098855328e-08\n",
      "Training Circuit at Epoch 124/400; Loss: 5.0121429784333316e-08\n",
      "Training Circuit at Epoch 125/400; Loss: 5.851270135082132e-08\n",
      "Training Circuit at Epoch 126/400; Loss: 6.6196405135166e-08\n",
      "Training Circuit at Epoch 127/400; Loss: 7.29937176258133e-08\n",
      "Training Circuit at Epoch 128/400; Loss: 7.868276830080845e-08\n",
      "Training Circuit at Epoch 129/400; Loss: 8.327701062071924e-08\n",
      "Training Circuit at Epoch 130/400; Loss: 8.682863983366929e-08\n",
      "Training Circuit at Epoch 131/400; Loss: 8.926396366515377e-08\n",
      "Training Circuit at Epoch 132/400; Loss: 9.05850880972281e-08\n",
      "Training Circuit at Epoch 133/400; Loss: 9.090509323073093e-08\n",
      "Training Circuit at Epoch 134/400; Loss: 9.028360858920337e-08\n",
      "Training Circuit at Epoch 135/400; Loss: 8.878597335115757e-08\n",
      "Training Circuit at Epoch 136/400; Loss: 8.657022609881437e-08\n",
      "Training Circuit at Epoch 137/400; Loss: 8.376010918187404e-08\n",
      "Training Circuit at Epoch 138/400; Loss: 8.041332522079614e-08\n",
      "Training Circuit at Epoch 139/400; Loss: 7.663850110084525e-08\n",
      "Training Circuit at Epoch 140/400; Loss: 7.256270451172497e-08\n",
      "Training Circuit at Epoch 141/400; Loss: 6.824571152641568e-08\n",
      "Training Circuit at Epoch 142/400; Loss: 6.374351391169597e-08\n",
      "Training Circuit at Epoch 143/400; Loss: 5.91489073276108e-08\n",
      "Training Circuit at Epoch 144/400; Loss: 5.45317974109949e-08\n",
      "Training Circuit at Epoch 145/400; Loss: 4.994367008848144e-08\n",
      "Training Circuit at Epoch 146/400; Loss: 4.545312615178432e-08\n",
      "Training Circuit at Epoch 147/400; Loss: 4.1111343440647374e-08\n",
      "Training Circuit at Epoch 148/400; Loss: 3.694240990892439e-08\n",
      "Training Circuit at Epoch 149/400; Loss: 3.298036777010793e-08\n",
      "Training Circuit at Epoch 150/400; Loss: 2.9255152633744785e-08\n",
      "Training Circuit at Epoch 151/400; Loss: 2.576942359500123e-08\n",
      "Training Circuit at Epoch 152/400; Loss: 2.2526953080692635e-08\n",
      "Training Circuit at Epoch 153/400; Loss: 1.9541924412003198e-08\n",
      "Training Circuit at Epoch 154/400; Loss: 1.6816016912279963e-08\n",
      "Training Circuit at Epoch 155/400; Loss: 1.4344129661836291e-08\n",
      "Training Circuit at Epoch 156/400; Loss: 1.2125562331988249e-08\n",
      "Training Circuit at Epoch 157/400; Loss: 1.0152106710847875e-08\n",
      "Training Circuit at Epoch 158/400; Loss: 8.409397067588031e-09\n",
      "Training Circuit at Epoch 159/400; Loss: 6.886625159019388e-09\n",
      "Training Circuit at Epoch 160/400; Loss: 5.568369099151482e-09\n",
      "Training Circuit at Epoch 161/400; Loss: 4.434111078310821e-09\n",
      "Training Circuit at Epoch 162/400; Loss: 3.4697046347176297e-09\n",
      "Training Circuit at Epoch 163/400; Loss: 2.6625373017452603e-09\n",
      "Training Circuit at Epoch 164/400; Loss: 1.995056786441296e-09\n",
      "Training Circuit at Epoch 165/400; Loss: 1.4522083535695174e-09\n",
      "Training Circuit at Epoch 166/400; Loss: 1.0213740964104545e-09\n",
      "Training Circuit at Epoch 167/400; Loss: 6.870788382684623e-10\n",
      "Training Circuit at Epoch 168/400; Loss: 4.3519676751202496e-10\n",
      "Training Circuit at Epoch 169/400; Loss: 2.536020282661866e-10\n",
      "Training Circuit at Epoch 170/400; Loss: 1.2896572698650743e-10\n",
      "Training Circuit at Epoch 171/400; Loss: 5.082356757668549e-11\n",
      "Training Circuit at Epoch 172/400; Loss: 1.1572298674877857e-11\n",
      "Training Circuit at Epoch 173/400; Loss: 2.0954349366775205e-12\n",
      "Training Circuit at Epoch 174/400; Loss: 1.486033518460772e-11\n",
      "Training Circuit at Epoch 175/400; Loss: 4.48803216812621e-11\n",
      "Training Circuit at Epoch 176/400; Loss: 8.606804158262094e-11\n",
      "Training Circuit at Epoch 177/400; Loss: 1.3319878533479823e-10\n",
      "Training Circuit at Epoch 178/400; Loss: 1.828672768766637e-10\n",
      "Training Circuit at Epoch 179/400; Loss: 2.3139978821973273e-10\n",
      "Training Circuit at Epoch 180/400; Loss: 2.765988549313647e-10\n",
      "Training Circuit at Epoch 181/400; Loss: 3.1745395201454585e-10\n",
      "Training Circuit at Epoch 182/400; Loss: 3.5224156924584804e-10\n",
      "Training Circuit at Epoch 183/400; Loss: 3.804134784957114e-10\n",
      "Training Circuit at Epoch 184/400; Loss: 4.021206700954849e-10\n",
      "Training Circuit at Epoch 185/400; Loss: 4.166820222195611e-10\n",
      "Training Circuit at Epoch 186/400; Loss: 4.2421821611071664e-10\n",
      "Training Circuit at Epoch 187/400; Loss: 4.2527048549345636e-10\n",
      "Training Circuit at Epoch 188/400; Loss: 4.200866321468766e-10\n",
      "Training Circuit at Epoch 189/400; Loss: 4.096203376491303e-10\n",
      "Training Circuit at Epoch 190/400; Loss: 3.946716287117624e-10\n",
      "Training Circuit at Epoch 191/400; Loss: 3.758270361586824e-10\n",
      "Training Circuit at Epoch 192/400; Loss: 3.5417624388855984e-10\n",
      "Training Circuit at Epoch 193/400; Loss: 3.3034619484340055e-10\n",
      "Training Circuit at Epoch 194/400; Loss: 3.04818281726682e-10\n",
      "Training Circuit at Epoch 195/400; Loss: 2.7840729721617663e-10\n",
      "Training Circuit at Epoch 196/400; Loss: 2.5160529215639826e-10\n",
      "Training Circuit at Epoch 197/400; Loss: 2.2496160489993144e-10\n",
      "Training Circuit at Epoch 198/400; Loss: 1.9904911052748275e-10\n",
      "Training Circuit at Epoch 199/400; Loss: 1.7416001973913353e-10\n",
      "Training Circuit at Epoch 200/400; Loss: 1.507101110576059e-10\n",
      "Training Circuit at Epoch 201/400; Loss: 1.2892131806552243e-10\n",
      "Training Circuit at Epoch 202/400; Loss: 1.0883016710039328e-10\n",
      "Training Circuit at Epoch 203/400; Loss: 9.06157371360905e-11\n",
      "Training Circuit at Epoch 204/400; Loss: 7.429135084890959e-11\n",
      "Training Circuit at Epoch 205/400; Loss: 5.985545392661606e-11\n",
      "Training Circuit at Epoch 206/400; Loss: 4.732103597859805e-11\n",
      "Training Circuit at Epoch 207/400; Loss: 3.658584546428756e-11\n",
      "Training Circuit at Epoch 208/400; Loss: 2.759303896482379e-11\n",
      "Training Circuit at Epoch 209/400; Loss: 2.0195844996351298e-11\n",
      "Training Circuit at Epoch 210/400; Loss: 1.4212186982831554e-11\n",
      "Training Circuit at Epoch 211/400; Loss: 9.524603328259218e-12\n",
      "Training Circuit at Epoch 212/400; Loss: 5.955902437904115e-12\n",
      "Training Circuit at Epoch 213/400; Loss: 3.3666402998733247e-12\n",
      "Training Circuit at Epoch 214/400; Loss: 1.6184831252985532e-12\n",
      "Training Circuit at Epoch 215/400; Loss: 5.666578317686799e-13\n",
      "Training Circuit at Epoch 216/400; Loss: 9.481304630298837e-14\n",
      "Training Circuit at Epoch 217/400; Loss: 5.284661597215745e-14\n",
      "Training Circuit at Epoch 218/400; Loss: 3.3295588508508445e-13\n",
      "Training Circuit at Epoch 219/400; Loss: 8.393286066166183e-13\n",
      "Training Circuit at Epoch 220/400; Loss: 1.4788170688007085e-12\n",
      "Training Circuit at Epoch 221/400; Loss: 2.192912518239609e-12\n",
      "Training Circuit at Epoch 222/400; Loss: 2.918887354041999e-12\n",
      "Training Circuit at Epoch 223/400; Loss: 3.6215475063272606e-12\n",
      "Training Circuit at Epoch 224/400; Loss: 4.259814723184263e-12\n",
      "Training Circuit at Epoch 225/400; Loss: 4.7981618678250015e-12\n",
      "Training Circuit at Epoch 226/400; Loss: 5.2300386244041874e-12\n",
      "Training Circuit at Epoch 227/400; Loss: 5.537348357620431e-12\n",
      "Training Circuit at Epoch 228/400; Loss: 5.724309914967307e-12\n",
      "Training Circuit at Epoch 229/400; Loss: 5.794031920913767e-12\n",
      "Training Circuit at Epoch 230/400; Loss: 5.758504784125762e-12\n",
      "Training Circuit at Epoch 231/400; Loss: 5.629163801756931e-12\n",
      "Training Circuit at Epoch 232/400; Loss: 5.410449865905775e-12\n",
      "Training Circuit at Epoch 233/400; Loss: 5.125788682391885e-12\n",
      "Training Circuit at Epoch 234/400; Loss: 4.780398299430999e-12\n",
      "Training Circuit at Epoch 235/400; Loss: 4.39515090988607e-12\n",
      "Training Circuit at Epoch 236/400; Loss: 3.983480212355062e-12\n",
      "Training Circuit at Epoch 237/400; Loss: 3.558930927738402e-12\n",
      "Training Circuit at Epoch 238/400; Loss: 3.1370461783808423e-12\n",
      "Training Circuit at Epoch 239/400; Loss: 2.7213786779611837e-12\n",
      "Training Circuit at Epoch 240/400; Loss: 2.325695191984778e-12\n",
      "Training Circuit at Epoch 241/400; Loss: 1.9528823003156504e-12\n",
      "Training Circuit at Epoch 242/400; Loss: 1.6090462295892394e-12\n",
      "Training Circuit at Epoch 243/400; Loss: 1.2996270726262082e-12\n",
      "Training Circuit at Epoch 244/400; Loss: 1.0249578963339445e-12\n",
      "Training Circuit at Epoch 245/400; Loss: 7.890355036010988e-13\n",
      "Training Circuit at Epoch 246/400; Loss: 5.883071807488705e-13\n",
      "Training Circuit at Epoch 247/400; Loss: 4.2310599468464716e-13\n",
      "Training Circuit at Epoch 248/400; Loss: 2.8987923172962837e-13\n",
      "Training Circuit at Epoch 249/400; Loss: 1.8607337892717624e-13\n",
      "Training Circuit at Epoch 250/400; Loss: 1.0857981180834031e-13\n",
      "Training Circuit at Epoch 251/400; Loss: 5.440092820663267e-14\n",
      "Training Circuit at Epoch 252/400; Loss: 2.2648549702353193e-14\n",
      "Training Circuit at Epoch 253/400; Loss: 4.551914400963142e-15\n",
      "Training Circuit at Epoch 254/400; Loss: 1.6653345369377348e-15\n",
      "Training Circuit at Epoch 255/400; Loss: 5.662137425588298e-15\n",
      "Training Circuit at Epoch 256/400; Loss: 1.84297022087776e-14\n",
      "Training Circuit at Epoch 257/400; Loss: 3.530509218307998e-14\n",
      "Training Circuit at Epoch 258/400; Loss: 5.5289106626332796e-14\n",
      "Training Circuit at Epoch 259/400; Loss: 7.527312106958561e-14\n",
      "Training Circuit at Epoch 260/400; Loss: 9.381384558082573e-14\n",
      "Training Circuit at Epoch 261/400; Loss: 1.1024514634527804e-13\n",
      "Training Circuit at Epoch 262/400; Loss: 1.2434497875801753e-13\n",
      "Training Circuit at Epoch 263/400; Loss: 1.3522516439934407e-13\n",
      "Training Circuit at Epoch 264/400; Loss: 1.4144241333724494e-13\n",
      "Training Circuit at Epoch 265/400; Loss: 1.446620601086579e-13\n",
      "Training Circuit at Epoch 266/400; Loss: 1.4432899320127035e-13\n",
      "Training Circuit at Epoch 267/400; Loss: 1.4233059175694507e-13\n",
      "Training Circuit at Epoch 268/400; Loss: 1.354472090042691e-13\n",
      "Training Circuit at Epoch 269/400; Loss: 1.27675647831893e-13\n",
      "Training Circuit at Epoch 270/400; Loss: 1.1746159600534156e-13\n",
      "Training Circuit at Epoch 271/400; Loss: 1.0658141036401503e-13\n",
      "Training Circuit at Epoch 272/400; Loss: 9.50350909079134e-14\n",
      "Training Circuit at Epoch 273/400; Loss: 8.293365993949919e-14\n",
      "Training Circuit at Epoch 274/400; Loss: 7.138734048339757e-14\n",
      "Training Circuit at Epoch 275/400; Loss: 5.950795411990839e-14\n",
      "Training Circuit at Epoch 276/400; Loss: 4.873879078104437e-14\n",
      "Training Circuit at Epoch 277/400; Loss: 3.930189507173054e-14\n",
      "Training Circuit at Epoch 278/400; Loss: 3.0531133177191805e-14\n",
      "Training Circuit at Epoch 279/400; Loss: 2.2648549702353193e-14\n",
      "Training Circuit at Epoch 280/400; Loss: 1.5987211554602254e-14\n",
      "Training Circuit at Epoch 281/400; Loss: 1.176836406102666e-14\n",
      "Training Circuit at Epoch 282/400; Loss: 6.5503158452884236e-15\n",
      "Training Circuit at Epoch 283/400; Loss: 4.3298697960381105e-15\n",
      "Training Circuit at Epoch 284/400; Loss: 1.7763568394002505e-15\n",
      "Training Circuit at Epoch 285/400; Loss: 1.5543122344752192e-15\n",
      "Training Circuit at Epoch 286/400; Loss: 2.220446049250313e-16\n",
      "Early Stopping at Epoch: 285\n"
     ]
    }
   ],
   "source": [
    "final_params, loss_list = mcts.circuitModelTuning(\n",
    "        initial_params=params,\n",
    "        model=qml_models.ToffoliQMLNoiseless,\n",
    "        num_epochs=400,\n",
    "        k=res_dict['k'],\n",
    "        op_pool=pool,\n",
    "        opt_callable=qml.AdamOptimizer,\n",
    "        lr=0.01,\n",
    "        grad_noise_factor=0,\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('CNOT', [0, 2], None), ('CRot', [1, 2], [1.1296803381386924, 1.5707963230898614, -4.693363452215963e-09]), ('CNOT', [0, 2], None), ('CRot', [1, 2], [-1.1930496409647117e-08, -1.5707963411733787, 0.9152925328474281]), ('Rot', [1], [-0.7463000267728153, 1.8594707168136634e-10, 0.7427270843683094]), ('PlaceHolder', [1], None), ('Rot', [0], [-0.7774424773033537, -6.058753607240465e-10, -0.3979643420400851]), ('PlaceHolder', [0], None), ('Rot', [2], [-0.9075802096245182, 5.806605976212192e-09, 0.6860885225524607]), ('PlaceHolder', [2], None), ('PlaceHolder', [0], None), ('PlaceHolder', [1], None), ('PlaceHolder', [2], None), ('PlaceHolder', [2], None), ('PlaceHolder', [0], None)]\n"
     ]
    }
   ],
   "source": [
    "model2 = qml_models.ToffoliQMLNoiseless(final_params.shape[0], final_params.shape[1], final_params.shape[2], res_dict['k'], pool)\n",
    "print(model2.toList(final_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0: ──╭QubitStateVector(M0)──╭C───────────────────────╭C───Rot(-0.777, 0, -0.398)─────────────────────────╭┤ State \n",
      " 1: ──├QubitStateVector(M0)──│───╭C───────────────────│───╭C───────────────────────Rot(-0.746, 0, 0.743)──├┤ State \n",
      " 2: ──╰QubitStateVector(M0)──╰X──╰Rot(1.13, 1.57, 0)──╰X──╰Rot(0, -1.57, 0.915)────Rot(-0.908, 0, 0.686)──╰┤ State \n",
      "M0 =\n",
      "[0 0 0 0 0 0 1 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@qml.qnode(dev)\n",
    "def searched_circ(x):\n",
    "    qml.QubitStateVector(x, wires=[0,1,2])\n",
    "    qml.CNOT(wires =[0,2])\n",
    "    qml.CRot(1.1296803381386924, 1.5707963230898614, 0, wires=[1,2])\n",
    "    qml.CNOT(wires =[0,2])\n",
    "    qml.CRot(0, -1.5707963411733787, 0.9152925328474281, wires=[1,2])\n",
    "    qml.Rot(-0.7463000267728153, 0, 0.7427270843683094, wires = 1)\n",
    "    qml.Rot(-0.7774424773033537, 0, -0.3979643420400851, wires = 0)\n",
    "    qml.Rot(-0.9075802096245182, 0, 0.6860885225524607, wires = 2)\n",
    "    return qml.state()\n",
    "\n",
    "print(qml.draw(searched_circ)(np.kron(computational_bases['1'], np.kron(computational_bases['1'], computational_bases['0']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State:  000\n",
      "State Vector: \n",
      "[1 0 0 0 0 0 0 0]\n",
      "Searched Circ Output:\n",
      "[0.7647+0.6444j 0.    +0.j     0.    +0.j     0.    +0.j     0.    +0.j     0.    +0.j     0.    +0.j     0.    +0.j    ]\n",
      "Target Circ Output:\n",
      "[1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      "\n",
      "State:  001\n",
      "State Vector: \n",
      "[0 1 0 0 0 0 0 0]\n",
      "Searched Circ Output:\n",
      "[0.    +0.j     0.8876+0.4607j 0.    +0.j     0.    +0.j     0.    +0.j     0.    +0.j     0.    +0.j     0.    +0.j    ]\n",
      "Target Circ Output:\n",
      "[0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      "\n",
      "State:  010\n",
      "State Vector: \n",
      "[0 0 1 0 0 0 0 0]\n",
      "Searched Circ Output:\n",
      "[ 0.0000e+00+0.0000e+00j  0.0000e+00+0.0000e+00j  9.4739e-01-3.2009e-01j -8.4365e-09-3.2526e-09j  0.0000e+00+0.0000e+00j  0.0000e+00+0.0000e+00j  0.0000e+00+0.0000e+00j  0.0000e+00+0.0000e+00j]\n",
      "Target Circ Output:\n",
      "[0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      "\n",
      "State:  011\n",
      "State Vector: \n",
      "[0 0 0 1 0 0 0 0]\n",
      "Searched Circ Output:\n",
      "[0.0000e+00+0.0000e+00j 0.0000e+00+0.0000e+00j 6.2744e-09+6.5104e-09j 7.3074e-02+9.9733e-01j 0.0000e+00+0.0000e+00j 0.0000e+00+0.0000e+00j 0.0000e+00+0.0000e+00j 0.0000e+00+0.0000e+00j]\n",
      "Target Circ Output:\n",
      "[0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      "\n",
      "State:  100\n",
      "State Vector: \n",
      "[0 0 0 0 1 0 0 0]\n",
      "Searched Circ Output:\n",
      "[0.    +0.j     0.    +0.j     0.    +0.j     0.    +0.j     0.8892-0.4575j 0.    +0.j     0.    +0.j     0.    +0.j    ]\n",
      "Target Circ Output:\n",
      "[0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j 0.+0.j]\n",
      "\n",
      "State:  101\n",
      "State Vector: \n",
      "[0 0 0 0 0 1 0 0]\n",
      "Searched Circ Output:\n",
      "[0.   +0.j     0.   +0.j     0.   +0.j     0.   +0.j     0.   +0.j     0.767-0.6417j 0.   +0.j     0.   +0.j    ]\n",
      "Target Circ Output:\n",
      "[0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j 0.+0.j]\n",
      "\n",
      "State:  110\n",
      "State Vector: \n",
      "[0 0 0 0 0 0 1 0]\n",
      "Searched Circ Output:\n",
      "[ 0.0000e+00+0.0000e+00j  0.0000e+00+0.0000e+00j  0.0000e+00+0.0000e+00j  0.0000e+00+0.0000e+00j  0.0000e+00+0.0000e+00j  0.0000e+00+0.0000e+00j -4.9726e-09+1.9376e-09j -9.4853e-01-3.1670e-01j]\n",
      "Target Circ Output:\n",
      "[0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j]\n",
      "\n",
      "State:  111\n",
      "State Vector: \n",
      "[0 0 0 0 0 0 0 1]\n",
      "Searched Circ Output:\n",
      "[ 0.0000e+00+0.0000e+00j  0.0000e+00+0.0000e+00j  0.0000e+00+0.0000e+00j  0.0000e+00+0.0000e+00j  0.0000e+00+0.0000e+00j  0.0000e+00+0.0000e+00j  6.9510e-02-9.9758e-01j -3.6896e-09+3.8558e-09j]\n",
      "Target Circ Output:\n",
      "[0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 0.+0.j 1.+0.j 0.+0.j]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for a in computational_bases.keys():\n",
    "    for b in computational_bases.keys():\n",
    "        for c in computational_bases.keys():\n",
    "            state_name = a+b+c\n",
    "            state = np.kron(np.kron(computational_bases[a], computational_bases[b]), computational_bases[c])\n",
    "            print(\"State: \", state_name)\n",
    "            print(\"State Vector: \")\n",
    "            print(state)\n",
    "            print(\"Searched Circ Output:\")\n",
    "            print(searched_circ(state))\n",
    "            print(\"Target Circ Output:\")\n",
    "            print(target_circ(state))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "48cd5b442f016699da984206989fd2d7e7204285b15197fe93b8704a3e807965"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('QuantumResearch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
